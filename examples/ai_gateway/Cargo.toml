[package]
name = "candle-vllm-ai-gateway-example"
version = "0.4.5"
edition = "2021"
license = "MIT"
description = "AI Gateway example demonstrating multi-backend routing with candle-vllm"
publish = false

[dependencies]
# Local candle-vllm crates
candle-vllm-openai = { path = "../../crates/candle-vllm-openai" }
candle-vllm-core = { path = "../../crates/candle-vllm-core" }

# Async runtime
tokio = { version = "1.38.0", features = ["rt-multi-thread", "macros", "sync"] }

# HTTP server
axum = { version = "0.8.7", features = ["tokio"] }
tower-http = { version = "0.6.7", features = ["cors", "trace"] }

# Serialization
serde = { version = "1.0.190", features = ["derive"] }
serde_json = "1.0.108"

# Utilities
tracing = "0.1.40"
tracing-subscriber = { version = "0.3.18", features = ["env-filter"] }
anyhow = "1.0.75"
uuid = { version = "1.5.0", features = ["v4"] }

[features]
default = []
cuda = ["candle-vllm-openai/cuda", "candle-vllm-core/cuda"]
metal = ["candle-vllm-openai/metal", "candle-vllm-core/metal"]
