# Example environment variables for candle-vllm
#
# Copy this file to .env and customize as needed:
#   cp .example.env .env
#
# Or export variables directly in your shell:
#   export CANDLE_VLLM_MCP_CONFIG=/path/to/mcp.json
#
# Note: .env files are not automatically loaded by candle-vllm.
# You must export these variables in your shell or use a tool like `dotenv`.

# ============================================================================
# MCP Configuration
# ============================================================================

# Port for MCP HTTP proxy/gateway (default: 3000)
# Used when converting command-based MCP servers to HTTP endpoints
# This is the port where your MCP gateway or proxy is running
# Example: If running an MCP gateway on port 3001:
#   MCP_PROXY_PORT=3001
MCP_PROXY_PORT=3000

# ============================================================================
# Configuration File Paths
# ============================================================================

# Path to MCP configuration file (mcp.json)
# Default: ./mcp.json (if exists)
# Priority: CLI --mcp-config > CANDLE_VLLM_MCP_CONFIG > ./mcp.json
# The MCP config file supports two formats:
#   1. {"servers": [{"name": "...", "url": "...", "auth": null, "timeout_secs": 30}]}
#   2. {"mcpServers": {"server-name": {"type": "node", "command": "...", "url": "..."}}}
# For command-based servers without URL, they will be converted to HTTP URLs
# using MCP_PROXY_PORT (default: http://localhost:3000/server-name)
CANDLE_VLLM_MCP_CONFIG=/path/to/mcp.json

# Path to models registry configuration file (models.yaml)
# Default: ./models.yaml or ./models.yml (if exists)
# Priority: CANDLE_VLLM_MODELS_CONFIG > ./models.yaml > ./models.yml
# This file defines model aliases and their configurations.
# See example.models.yaml for the complete structure.
CANDLE_VLLM_MODELS_CONFIG=/path/to/models.yaml

# ============================================================================
# Testing Environment Variables
# ============================================================================

# Path to test model directory (for running integration tests)
# Used by test suites to locate models for testing
# Example: /path/to/mistral-7b-instruct
CANDLE_VLLM_TEST_MODEL=/path/to/test/model

# Device to use for tests (cpu, cuda, metal)
# Default: cpu
# Examples:
#   CANDLE_VLLM_TEST_DEVICE=cpu
#   CANDLE_VLLM_TEST_DEVICE=cuda
#   CANDLE_VLLM_TEST_DEVICE=metal
CANDLE_VLLM_TEST_DEVICE=cpu

# ============================================================================
# Logging Configuration
# ============================================================================

# Rust logging level (error, warn, info, debug, trace)
# Controls verbosity of candle-vllm logs
# Default: info (if --log flag not set)
# Examples:
#   RUST_LOG=error    # Only errors
#   RUST_LOG=warn     # Warnings and errors
#   RUST_LOG=info     # Info, warnings, and errors
#   RUST_LOG=debug    # Debug info and above
#   RUST_LOG=trace    # All logs (very verbose)
# You can also set per-module levels:
#   RUST_LOG=candle_vllm_server=debug,candle_vllm_core=info
RUST_LOG=info

# ============================================================================
# Hugging Face Configuration
# ============================================================================

# Hugging Face authentication token
# Used for downloading private models or models requiring authentication
# Alternative: Use --hf-token CLI flag or --hf-token-path for file-based auth
# Default: ~/.cache/huggingface/token (if file exists)
# Priority: CLI --hf-token > CLI --hf-token-path > HF_TOKEN env var > ~/.cache/huggingface/token
HF_TOKEN=your_huggingface_token_here

# ============================================================================
# Server Configuration (Advanced)
# ============================================================================

# Keep-alive interval for streaming responses (milliseconds)
# Controls how often keep-alive messages are sent during streaming
# Default: 100 milliseconds
# Note: This may be overridden by server configuration
KEEP_ALIVE_INTERVAL=100

# ============================================================================
# Prompt Cache Configuration
# ============================================================================

# Enable prompt caching to cache and reuse KV cache for common prefixes
# This can significantly speed up inference for repeated prompts
# Default: false (disabled)
# Priority: CLI --prompt-cache > CANDLE_VLLM_PROMPT_CACHE_ENABLED > models.yaml
CANDLE_VLLM_PROMPT_CACHE_ENABLED=true

# Backend selection for prompt cache storage
# Options: memory, sled, redis
# - memory: In-memory storage (fast, no persistence)
# - sled: Embedded database (persistent, recommended for production)
# - redis: Redis server (distributed, good for multi-instance deployments)
# Default: memory
# Priority: CLI --prompt-cache-backend > CANDLE_VLLM_PROMPT_CACHE_BACKEND > models.yaml
CANDLE_VLLM_PROMPT_CACHE_BACKEND=sled

# Storage path for sled backend
# Only used when backend is 'sled'
# Default: ~/.candle-vllm/cache
# Priority: CLI --prompt-cache-path > CANDLE_VLLM_PROMPT_CACHE_PATH > models.yaml
CANDLE_VLLM_PROMPT_CACHE_PATH=~/.candle-vllm/cache

# Redis URL for redis backend
# Only used when backend is 'redis'
# Format: redis://[username:password@]host:port[/database]
# Examples:
#   redis://localhost:6379
#   redis://user:pass@redis.example.com:6379/0
# Priority: CLI --prompt-cache-redis-url > CANDLE_VLLM_PROMPT_CACHE_REDIS_URL > models.yaml
CANDLE_VLLM_PROMPT_CACHE_REDIS_URL=redis://localhost:6379

# TTL (Time To Live) in seconds for cached entries
# Only used when backend is 'redis'
# After this time, cached entries will expire
# Default: None (no expiration)
# Priority: CLI --prompt-cache-ttl > CANDLE_VLLM_PROMPT_CACHE_TTL > models.yaml
CANDLE_VLLM_PROMPT_CACHE_TTL=3600

# Maximum number of cached prefixes to store
# Only used when backend is 'memory'
# When limit is reached, least recently used entries are evicted
# Default: None (no limit)
# Priority: CLI --prompt-cache-max-prefixes > CANDLE_VLLM_PROMPT_CACHE_MAX_PREFIXES > models.yaml
CANDLE_VLLM_PROMPT_CACHE_MAX_PREFIXES=10000

# Minimum prefix length (in tokens) to cache
# Shorter prefixes will not be cached
# Default: 16
# Priority: CLI --prompt-cache-min-length > CANDLE_VLLM_PROMPT_CACHE_MIN_LENGTH > models.yaml
CANDLE_VLLM_PROMPT_CACHE_MIN_LENGTH=16

# ============================================================================
# CUDA/GPU Configuration (if applicable)
# ============================================================================

# CUDA device visibility
# Controls which CUDA devices are visible to the application
# Example: CUDA_VISIBLE_DEVICES=0,1  # Only use GPUs 0 and 1
# CUDA_VISIBLE_DEVICES=0

# ============================================================================
# Notes
# ============================================================================
#
# 1. Configuration Priority:
#    - CLI arguments take highest priority
#    - Environment variables override defaults
#    - Default file locations are checked last
#
# 2. MCP Tools Auto-Injection:
#    - When CANDLE_VLLM_MCP_CONFIG points to a valid mcp.json with servers,
#      tools are automatically injected into chat completion requests
#    - Tools are only injected if the request doesn't explicitly specify tools
#    - MCP servers can be defined in two formats:
#      a) {"servers": [...]} - Recommended format with explicit URLs
#      b) {"mcpServers": {...}} - Claude Desktop compatible format
#    - Command-based servers (without URL) are converted to HTTP URLs using MCP_PROXY_PORT
#
# 3. Models Configuration:
#    - CANDLE_VLLM_MODELS_CONFIG allows you to use model aliases from models.yaml
#    - Example: cargo run --release --features metal -- --m mistral-7b
#      (where mistral-7b is defined in models.yaml)
#    - Models can be defined with hf_id (Hugging Face) or local_path (local files)
#    - All model parameters (dtype, temperature, etc.) go under the "params" key
#
# 4. Testing:
#    - Set CANDLE_VLLM_TEST_MODEL to run integration tests with actual models
#    - Set CANDLE_VLLM_TEST_DEVICE to test on specific hardware
#    - Tests will skip gracefully if CANDLE_VLLM_TEST_MODEL is not set
#
# 5. Logging:
#    - RUST_LOG controls the verbosity of all Rust logging
#    - Use RUST_LOG=debug for troubleshooting
#    - Use RUST_LOG=warn for production (less verbose)
#    - Can set per-module levels: RUST_LOG=candle_vllm_server=debug
#
# 6. Hugging Face:
#    - HF_TOKEN is required for private models
#    - Can also be set via CLI: --hf-token or --hf-token-path
#    - Default location: ~/.cache/huggingface/token
#    - Priority: CLI > Environment > Default file location
#
# 7. Prompt Caching:
#    - CANDLE_VLLM_PROMPT_CACHE_ENABLED enables prompt prefix caching
#    - Caching can significantly speed up repeated prompts (system prompts, RAG context)
#    - Choose backend based on your needs:
#      * memory: Fast, no persistence, good for development
#      * sled: Persistent, embedded, recommended for production
#      * redis: Distributed, good for multi-instance deployments
#    - Configure cache_path for sled or redis_url for redis
#    - Set min_prefix_length to avoid caching very short prefixes
#    - See docs/CONFIGURATION.md for detailed prompt caching configuration
