# Example environment variables for candle-vllm
#
# Copy this file to .env and customize as needed:
#   cp .example.env .env
#
# Or export variables directly in your shell:
#   export CANDLE_VLLM_MCP_CONFIG=/path/to/mcp.json
#
# Note: .env files are not automatically loaded by candle-vllm.
# You must export these variables in your shell or use a tool like `dotenv`.

# ============================================================================
# MCP Configuration
# ============================================================================

# Port for MCP HTTP proxy/gateway (default: 3000)
# Used when converting command-based MCP servers to HTTP endpoints
# This is the port where your MCP gateway or proxy is running
# Example: If running an MCP gateway on port 3001:
#   MCP_PROXY_PORT=3001
MCP_PROXY_PORT=3000

# ============================================================================
# Configuration File Paths
# ============================================================================

# Path to MCP configuration file (mcp.json)
# Default: ./mcp.json (if exists)
# Priority: CLI --mcp-config > CANDLE_VLLM_MCP_CONFIG > ./mcp.json
# The MCP config file supports two formats:
#   1. {"servers": [{"name": "...", "url": "...", "auth": null, "timeout_secs": 30}]}
#   2. {"mcpServers": {"server-name": {"type": "node", "command": "...", "url": "..."}}}
# For command-based servers without URL, they will be converted to HTTP URLs
# using MCP_PROXY_PORT (default: http://localhost:3000/server-name)
CANDLE_VLLM_MCP_CONFIG=/path/to/mcp.json

# Path to models registry configuration file (models.yaml)
# Default: ./models.yaml or ./models.yml (if exists)
# Priority: CANDLE_VLLM_MODELS_CONFIG > ./models.yaml > ./models.yml
# This file defines model aliases and their configurations.
# See example.models.yaml for the complete structure.
CANDLE_VLLM_MODELS_CONFIG=/path/to/models.yaml

# ============================================================================
# Testing Environment Variables
# ============================================================================

# Path to test model directory (for running integration tests)
# Used by test suites to locate models for testing
# Example: /path/to/mistral-7b-instruct
CANDLE_VLLM_TEST_MODEL=/path/to/test/model

# Device to use for tests (cpu, cuda, metal)
# Default: cpu
# Examples:
#   CANDLE_VLLM_TEST_DEVICE=cpu
#   CANDLE_VLLM_TEST_DEVICE=cuda
#   CANDLE_VLLM_TEST_DEVICE=metal
CANDLE_VLLM_TEST_DEVICE=cpu

# ============================================================================
# Logging Configuration
# ============================================================================

# Rust logging level (error, warn, info, debug, trace)
# Controls verbosity of candle-vllm logs
# Default: info (if --log flag not set)
# Examples:
#   RUST_LOG=error    # Only errors
#   RUST_LOG=warn     # Warnings and errors
#   RUST_LOG=info     # Info, warnings, and errors
#   RUST_LOG=debug    # Debug info and above
#   RUST_LOG=trace    # All logs (very verbose)
# You can also set per-module levels:
#   RUST_LOG=candle_vllm_server=debug,candle_vllm_core=info
RUST_LOG=info

# ============================================================================
# Hugging Face Configuration
# ============================================================================

# Hugging Face authentication token
# Used for downloading private models or models requiring authentication
# Alternative: Use --hf-token CLI flag or --hf-token-path for file-based auth
# Default: ~/.cache/huggingface/token (if file exists)
# Priority: CLI --hf-token > CLI --hf-token-path > HF_TOKEN env var > ~/.cache/huggingface/token
HF_TOKEN=your_huggingface_token_here

# ============================================================================
# Server Configuration (Advanced)
# ============================================================================

# Keep-alive interval for streaming responses (milliseconds)
# Controls how often keep-alive messages are sent during streaming
# Default: 100 milliseconds
# Note: This may be overridden by server configuration
KEEP_ALIVE_INTERVAL=100

# ============================================================================
# CUDA/GPU Configuration (if applicable)
# ============================================================================

# CUDA device visibility
# Controls which CUDA devices are visible to the application
# Example: CUDA_VISIBLE_DEVICES=0,1  # Only use GPUs 0 and 1
# CUDA_VISIBLE_DEVICES=0

# ============================================================================
# Notes
# ============================================================================
#
# 1. Configuration Priority:
#    - CLI arguments take highest priority
#    - Environment variables override defaults
#    - Default file locations are checked last
#
# 2. MCP Tools Auto-Injection:
#    - When CANDLE_VLLM_MCP_CONFIG points to a valid mcp.json with servers,
#      tools are automatically injected into chat completion requests
#    - Tools are only injected if the request doesn't explicitly specify tools
#    - MCP servers can be defined in two formats:
#      a) {"servers": [...]} - Recommended format with explicit URLs
#      b) {"mcpServers": {...}} - Claude Desktop compatible format
#    - Command-based servers (without URL) are converted to HTTP URLs using MCP_PROXY_PORT
#
# 3. Models Configuration:
#    - CANDLE_VLLM_MODELS_CONFIG allows you to use model aliases from models.yaml
#    - Example: cargo run --release --features metal -- --m mistral-7b
#      (where mistral-7b is defined in models.yaml)
#    - Models can be defined with hf_id (Hugging Face) or local_path (local files)
#    - All model parameters (dtype, temperature, etc.) go under the "params" key
#
# 4. Testing:
#    - Set CANDLE_VLLM_TEST_MODEL to run integration tests with actual models
#    - Set CANDLE_VLLM_TEST_DEVICE to test on specific hardware
#    - Tests will skip gracefully if CANDLE_VLLM_TEST_MODEL is not set
#
# 5. Logging:
#    - RUST_LOG controls the verbosity of all Rust logging
#    - Use RUST_LOG=debug for troubleshooting
#    - Use RUST_LOG=warn for production (less verbose)
#    - Can set per-module levels: RUST_LOG=candle_vllm_server=debug
#
# 6. Hugging Face:
#    - HF_TOKEN is required for private models
#    - Can also be set via CLI: --hf-token or --hf-token-path
#    - Default location: ~/.cache/huggingface/token
#    - Priority: CLI > Environment > Default file location
