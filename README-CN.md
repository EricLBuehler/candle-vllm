<p align="center">
    <img src="./res/candle_vllm_logo.png" alt="candle vLLM" width=55%/>
</p>

<p align="center">
  <a href="./README.md">English</a> |
  <a href="./README-CN.md">ç®€ä½“ä¸­æ–‡</a> |
</p>

[![Continuous integration](https://github.com/EricLBuehler/candle-vllm/actions/workflows/ci.yml/badge.svg)](https://github.com/EricLBuehler/candle-vllm/actions/workflows/ci.yml)

é«˜æ•ˆã€æ˜“ç”¨çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸æœåŠ¡å¹³å°ï¼Œæä¾›OpenAIå…¼å®¹çš„APIæœåŠ¡ã€‚

## åŠŸèƒ½ç‰¹æ€§
- æä¾›OpenAIå…¼å®¹çš„APIæœåŠ¡ï¼Œç”¨äºéƒ¨ç½²LLMã€‚
- é«˜åº¦å¯æ‰©å±•çš„åŸºäºtraitçš„ç³»ç»Ÿï¼Œæ”¯æŒå¿«é€Ÿå®ç°æ–°çš„æ¨¡å‹æœåŠ¡ã€‚
- ç”Ÿæˆè¿‡ç¨‹ä¸­æ”¯æŒæµå¼ï¼ˆstreamï¼‰ä¼ è¾“ã€‚
- ä½¿ç”¨Paged Attentioné«˜æ•ˆç®¡ç†KVç¼“å­˜ã€‚
- æŒç»­æ‰¹å¤„ç†ï¼ˆcontinuous batchingï¼Œä¸åŒæ—¶é—´æ®µçš„è¯·æ±‚decodingé˜¶æ®µèšåˆä¸ºæ‰¹é‡å¤„ç†ï¼‰ã€‚
- åŸä½ï¼ˆIn-situï¼‰é‡åŒ–ï¼ˆåŠåŸä½Marlinæ ¼å¼è½¬æ¢ï¼‰ã€‚
- æ”¯æŒ`GPTQ/Marlin`æ ¼å¼é‡åŒ–ï¼ˆ4ä½ï¼‰ã€‚
- æ”¯æŒ`Mac/Metal`è®¾å¤‡ã€‚
- æ”¯æŒ`å¤šGPU`æ¨ç†ï¼ˆåŒ…æ‹¬`å¤šè¿›ç¨‹`å’Œ`å¤šçº¿ç¨‹`æ¨¡å¼ï¼‰ã€‚
- æ”¯æŒ`å¤šèŠ‚ç‚¹`æ¨ç†ï¼ˆä½¿ç”¨MPIè¿è¡Œï¼‰ã€‚

## æ”¯æŒçš„æ¨¡å‹
- ç›®å‰ï¼Œcandle-vllmæ”¯æŒä»¥ä¸‹æ¨¡å‹ç»“æ„çš„æ¨ç†æœåŠ¡ã€‚
  <details>
    <summary>æ˜¾ç¤ºæ”¯æŒçš„æ¨¡å‹æ¶æ„</summary>

    | æ¨¡å‹ID | æ¨¡å‹ç±»å‹ | æ˜¯å¦æ”¯æŒ | é€Ÿåº¦ï¼ˆA100, `BF16`ï¼‰ | ååé‡ï¼ˆ`BF16`, `bs=16`ï¼‰ | é‡åŒ–ï¼ˆA100, `Q4K`æˆ–`Marlin`ï¼‰ | é‡åŒ–ååé‡ï¼ˆ`GTPQ/Marlin`, `bs=16`ï¼‰ |
    |--|--|--|--|--|--|--|
    | #1 | **LLAMA** |âœ…|65 tks/s (8B) | 553 tks/s (8B) | 75 tks/s (8B), 115 tks/s (8B, **Marlin**) |968 tks/s (8B)|
    | #2 | **Mistral** |âœ…|70 tks/s (7B)| 585 tks/s (7B) | 96 tks/s (7B), 115 tks/s (7B, **Marlin**) |981 tks/s (7B)|
    | #3 | **Phi** |âœ…|107 tks/s (3.8B)| 744 tks/s (3.8B)|135 tks/s (3.8B)|TBD|
    | #4 | **QWen2/Qwen3** |âœ…|81 tks/s (8B)|831 tks/s (8B) |-|TBD|S
    | #4 | **Yi** |âœ…|75 tks/s (6B)| 566 tks/s (6B) | 105 tks/s (6B)|TBD|
    | #5 | **StableLM** |âœ…|99 tks/s (3B)|TBD|-|TBD|
    | #6 | **Gemma-2/Gemma-3** |âœ…|60 tks/s (9B)|TBD |73 tks/s (9B, **Marlin**) |587 tks/s (9B)|
    | #7 | **DeepSeek-R1-Distill-QWen** |âœ…|48 tks (14B)|TBD|62 tks (14B)|TBD|
    | #8 | **DeepSeek-R1-Distill-LLaMa** |âœ…|65 tks (8B)|TBD|108 tks (8B)|TBD|
    | #9 | **DeepSeek V2/V3/R1** |âœ…|TBD|TBD|~20 tks **(AWQ 671B, tp=8, offloading)**|TBD|
    | #10 | **QwQ-32B** |âœ…|30 tks/s **(32B, tp=2)**|TBD |36 tks/s **(32B, Q4K, GGUF)**|TBD|
    | #11 | **GLM4** |âœ…|55 tks/s **(9B)**|TBD |92 tks/s **(9B, Q4K, GGUF)**|TBD|
  </details>

### æ¼”ç¤ºè§†é¢‘
- GPUä¸Apple Silicon
  <details>
    <summary>æ˜¾ç¤ºæ¼”ç¤ºè§†é¢‘</summary>

    **GPU**ï¼ˆA100, BF16, QWen3-8Bæ¨ç†æ¨¡å‹ï¼‰ä¸Šçš„èŠå¤©æ¼”ç¤º
    <img src="res/Qwen3-8B-Reasoning-A100.gif" width="85%" height="85%" >

    **Apple Silicon**ï¼ˆM4ï¼Œ16GBç»Ÿä¸€å†…å­˜ï¼ŒQ2K, QWen3-8Bï¼‰ä¸Šçš„èŠå¤©æ¼”ç¤º
    <img src="res/Qwen3-8B-Apple-M4.gif" width="85%" height="85%" >
  </details>

## åŸºæœ¬ç”¨æ³•
### æ„å»ºCandle-vLLM

```shell
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh #å®‰è£…Rustï¼Œéœ€è¦1.83.0åŠä»¥ä¸Šç‰ˆæœ¬
sudo apt install libssl-dev pkg-config -y
git clone git@github.com:EricLBuehler/candle-vllm.git
cd candle-vllm

#Mac/Metalå¹³å°ç¼–è¯‘å‘½ä»¤
cargo build --release --features metal

#CUDAå¹³å°ï¼šç¡®ä¿CUDA Toolkitåœ¨ç³»ç»ŸPATHä¸­
export PATH=$PATH:/usr/local/cuda/bin/

#CUDAå¹³å°ï¼šå•èŠ‚ç‚¹ï¼ˆå•æœºå•å¡æˆ–å•æœºå¤šå¡ï¼‰ç¼–è¯‘å‘½ä»¤
cargo build --release --features cuda,nccl

#CUDAå¹³å°ï¼šå•èŠ‚ç‚¹ï¼ˆä½¿ç”¨flash attention kernelï¼Œé€‚ç”¨äºé•¿ä¸Šä¸‹æ–‡æ¨ç†ï¼‰ç¼–è¯‘å‘½ä»¤
cargo build --release --features cuda,nccl,flash-attn

#CUDAå¹³å°ï¼šå¤šèŠ‚ç‚¹ï¼ˆå¤šæœºæ¨ç†ï¼‰ç¼–è¯‘å‘½ä»¤
sudo apt update
sudo apt install libopenmpi-dev openmpi-bin -y #å®‰è£…MPI
sudo apt install clang libclang-dev
cargo build --release --features cuda,nccl,mpi #åŒ…å«MPIåŠŸèƒ½
#æˆ–
cargo build --release --features cuda,nccl,flash-attn,mpi #åŒæ—¶åŒ…å«flash attentionä¸MPIåŠŸèƒ½
```

### æ„å»º/è¿è¡Œå‚æ•°

- [`ENV_PARAM`] cargo run [`BUILD_PARAM`] -- [`PROGRAM_PARAM`] [`MODEL_ID/MODEL_WEIGHT_PATH`] [`MODEL_TYPE`] [`MODEL_PARAM`]  
  <details>
    <summary>æ˜¾ç¤ºè¯¦æƒ…</summary>

    **ç¤ºä¾‹:**

    ```shell
    [RUST_LOG=warn] cargo run [--release --features cuda,nccl] -- [--multi-process --log --dtype bf16 --port 2000 --device-ids "0,1" --kvcache-mem-gpu 8192] [--weight-path /home/weights/Qwen3-27B-GPTQ-4Bit] [qwen3] [--quant gptq --temperature 0.7 --penalty 1.0 --top-k 32 --top-p 0.95 --thinking]
    ```

    `ENV_PARAM`: RUST_LOG=warn

    `BUILD_PARAM`: --release --features cuda,nccl

    `PROGRAM_PARAM`ï¼š--multi-process --log --dtype bf16 --port 2000 --device-ids "0,1" --kvcache-mem-gpu 8192

    `MODEL_WEIGHT_PATH`: --weight-path /home/weights/Qwen3-27B-GPTQ-4Bit

    `MODEL_TYPE`: qwen3

    `MODEL_PARAM`: --quant gptq --temperature 0.7 --penalty 1.0 --top-k 32 --top-p 0.95 --thinking

    å…¶ä¸­ï¼Œ`kvcache-mem-gpu`å‚æ•°æ§åˆ¶KV Cacheç¼“å­˜ï¼Œé•¿æ–‡æœ¬æˆ–æ‰¹é‡æ¨ç†é‡è¯·å¢å¤§ç¼“å­˜ï¼›`MODEL_TYPE`å¯é€‰å€¼ä¸ºï¼š["llama", "llama3", "mistral", "phi2", "phi3", "qwen2", "qwen3", "glm4", "gemma", "gemma3", "yi", "stable-lm", "deep-seek"]
  </details>

## å¦‚ä½•è¿è¡Œï¼Ÿ

- è¿è¡Œ**æœªå‹ç¼©**æ¨¡å‹ 
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **æœ¬åœ°è·¯å¾„**

    ```shell
    target/release/candle-vllm --port 2000 --weight-path /home/DeepSeek-R1-Distill-Llama-8B/ llama3 --temperature 0. --penalty 1.0
    ```

    **æ¨¡å‹IDï¼ˆä»Huggingfaceä¸‹è½½ï¼‰**

    ```shell
    target/release/candle-vllm --model-id deepseek-ai/DeepSeek-R1-0528-Qwen3-8B qwen3
    ```

  </details>

- è¿è¡Œ**GGUF**æ¨¡å‹ 
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **æœ¬åœ°è·¯å¾„ï¼ˆæŒ‡å®šç«¯å£ã€æ•°æ®ç±»å‹ã€é‡‡æ ·å‚æ•°ï¼‰**

    ```shell
    target/release/candle-vllm --port 2000 --dtype bf16 --weight-file /home/data/DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf qwen3 --quant gguf --temperature 0.7 --penalty 1.1
    ```

    **æ¨¡å‹IDï¼ˆä»Huggingfaceä¸‹è½½ï¼‰**

    ```shell
    target/release/candle-vllm --model-id unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF --weight-file DeepSeek-R1-0528-Qwen3-8B-Q2_K.gguf qwen3 --quant gguf
    ```

  </details>

- åœ¨**Apple Silicon**ä¸Šè¿è¡Œ**GGUF**æ¨¡å‹
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **æœ¬åœ°è·¯å¾„ï¼ˆå‡è®¾æ¨¡å‹å·²ä¸‹è½½åˆ°/homeï¼‰**

    ```shell
    cargo run --release --features metal -- --port 2000 --dtype bf16 --weight-file /home/qwq-32b-q4_k_m.gguf qwen2 --quant gguf --temperature 0. --penalty 1.0
    ```

    **æ¨¡å‹IDï¼ˆä»Huggingfaceä¸‹è½½ï¼‰**

    ```shell
    cargo run --release --features metal -- --port 2000 --dtype bf16 --model-id Qwen/QwQ-32B-GGUF --weight-file qwq-32b-q4_k_m.gguf qwen2 --quant gguf --temperature 0. --penalty 1.0
    ```

  </details>

- å°†æœªå‹ç¼©æ¨¡å‹ä½¿ç”¨**åŸä½ï¼ˆin-situï¼‰é‡åŒ–**åŠ è½½å¹¶è¿è¡Œä¸ºé‡åŒ–æ¨¡å‹
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **åªéœ€åœ¨è¿è¡Œæœªé‡åŒ–æ¨¡å‹æ—¶æ·»åŠ `quant`å‚æ•°**

    ```shell
    target/release/candle-vllm --port 2000 --weight-path /home/DeepSeek-R1-Distill-Llama-8B/ llama3 --quant q4k --temperature 0. --penalty 1.0
    ```

    æ³¨ï¼šåŸä½é‡åŒ–åŠ è½½å¯èƒ½éœ€è¦æ›´é•¿çš„åŠ è½½æ—¶é—´ï¼ŒåŸä½`quant`å‚æ•°é€‰é¡¹ï¼š["q4_0", "q4_1", "q5_0", "q5_1", "q8_0", "q2k", "q3k","q4k","q5k","q6k"]
  </details>

- è¿è¡Œ**Marlinå…¼å®¹çš„GPTQæ¨¡å‹**ï¼ˆ4ä½GPTQï¼Œ128åˆ†ç»„ï¼Œdesc_act=Falseï¼‰
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **æœ¬åœ°è·¯å¾„**

    ```shell
    target/release/candle-vllm --dtype bf16 --port 2000 --weight-path /home/DeepSeek-R1-Distill-Qwen-14B-GPTQ_4bit-128g qwen2 --quant gptq --temperature 0. --penalty 1.0
    ```

    **æ¨¡å‹IDï¼ˆä»Huggingfaceä¸‹è½½ï¼‰**

    ```shell
    target/release/candle-vllm --model-id thesven/Llama-3-8B-GPTQ-4bit llama3 --quant gptq
    ```

    **å°†æœªå‹ç¼©æ¨¡å‹è½¬æ¢ä¸ºMarlinå…¼å®¹æ ¼å¼**
    ```shell
    python3 examples/convert_marlin.py --src /home/DeepSeek-R1-Distill-Qwen-14B/ --dst /home/DeepSeek-R1-Distill-Qwen-14B-GPTQ_4bit-128g
    target/release/candle-vllm --dtype bf16 --port 2000 --weight-path /home/DeepSeek-R1-Distill-Qwen-14B-GPTQ_4bit-128g qwen2 --quant gptq --temperature 0. --penalty 1.0
    ```

  </details>

- è¿è¡Œ**Marlinå…¼å®¹çš„AWQæ¨¡å‹**
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **å°†AWQæ¨¡å‹è½¬æ¢ä¸ºMarlinå…¼å®¹æ ¼å¼**
    ```shell
    python3 examples/convert_awq_marlin.py --src /home/Meta-Llama-3.1-8B-Instruct-AWQ-INT4/ --dst /home/Meta-Llama-3.1-8B-Instruct-AWQ-INT4-Marlin/ --bits 4 --method awq --group 128 --nk False
    ```

    **è¿è¡Œè½¬æ¢åçš„AWQæ¨¡å‹**
    ```shell
    target/release/candle-vllm --multi-process --dtype f16 --port 2000 --device-ids "0" --weight-path /home/Meta-Llama-3.1-8B-Instruct-AWQ-INT4-Marlin/ llama3 --quant awq --temperature 0. --penalty 1.0
    ```

  </details>

- è¿è¡Œ**Marlinæ ¼å¼æ¨¡å‹**
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    ```shell
    target/release/candle-vllm --dtype bf16 --port 2000 --weight-path /home/DeepSeek-R1-Distill-Qwen-14B-GPTQ-Marlin/ qwen2 --quant marlin --penalty 1.0 --temperature 0.
    ```

  </details>

- ä½¿ç”¨**å¤šè¿›ç¨‹æ¨¡å¼ï¼ˆå¤šGPUï¼‰**è¿è¡Œ**å¤§å‹æ¨¡å‹**
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **åœ¨ä¸¤å—GPUä¸Šè¿è¡ŒQwQ-32B BF16æ¨¡å‹**
    ```shell
    cargo run --release --features cuda,nccl -- --multi-process --dtype bf16 --port 2000 --device-ids "0,1" --weight-path /home/QwQ-32B/ qwen2 --penalty 1.0 --temperature 0.
    ```

    **åœ¨ä¸¤å—GPUä¸Šè¿è¡ŒQwQ-32B 4ä½AWQæ¨¡å‹**

    1) å°†AWQæ¨¡å‹è½¬æ¢ä¸ºMarlinå…¼å®¹æ ¼å¼
    ```shell
    python3 examples/convert_awq_marlin.py --src /home/QwQ-32B-AWQ/ --dst /home/QwQ-32B-AWQ-Marlin/ --bits 4 --method awq --group 128 --nk False
    ```

    2) è¿è¡Œè½¬æ¢åçš„AWQæ¨¡å‹
    ```shell
    cargo run --release --features cuda,nccl -- --multi-process --dtype bf16 --port 2000 --device-ids "0,1" --weight-path /home/QwQ-32B-AWQ-Marlin/ qwen2 --quant awq --penalty 1.0 --temperature 0.
    ```

    **æ³¨æ„**ï¼šä½¿ç”¨çš„GPUæ•°é‡ï¼ˆ`--device-ids`ï¼‰å¿…é¡»ä¸º2çš„å¹‚æ¬¡æ–¹ï¼ˆä¾‹å¦‚2ã€4æˆ–8ï¼‰ã€‚
  </details>

- ä½¿ç”¨**å¤šçº¿ç¨‹æ¨¡å¼ï¼ˆå¤šGPUï¼Œè°ƒè¯•ç”¨é€”ï¼‰**è¿è¡Œ**å¤§å‹æ¨¡å‹**
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    åªéœ€ç§»é™¤`--multi-process`å‚æ•°ã€‚

    **åœ¨ä¸¤å—GPUä¸Šè¿è¡ŒQwQ-32B BF16æ¨¡å‹**
    ```shell
    cargo run --release --features cuda,nccl -- --dtype bf16 --port 2000 --device-ids "0,1" --weight-path /home/QwQ-32B/ qwen2 --penalty 1.0 --temperature 0.
    ```

    å¦‚æœåœ¨å¤šçº¿ç¨‹å¤šGPUæ¨¡å¼ä¸‹é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥å°è¯•ï¼š
    ```shell
    export NCCL_P2P_DISABLE=1 #åœ¨æŸäº›ç¯å¢ƒä¸­ç¦ç”¨P2PåŠŸèƒ½ä»¥é¿å…éæ³•å†…å­˜è®¿é—®
    ```

  </details>

- åœ¨**ä½æ˜¾å­˜GPUï¼ˆCPUå¸è½½ï¼‰**ä¸Šè¿è¡Œ**DeepSeek-R1ï¼ˆ671B/685Bï¼‰**
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **1. å°†DeepSeek-R1-AWQæ¨¡å‹è½¬æ¢ä¸ºMarlinå…¼å®¹æ ¼å¼**
    ```shell
    python3 examples/convert_awq_marlin.py --src /data/DeepSeek-R1-AWQ/ --dst /data/DeepSeek-R1-AWQ-Marlin/ 
    ```

    **2. åœ¨8å—A100ï¼ˆ40GBï¼‰ä¸Šè¿è¡ŒDeepSeek-R1æ¨¡å‹**
    ```shell
    cargo run --release --features cuda,nccl -- --log --multi-process --dtype bf16 --port 2000 --device-ids "0,1,2,3,4,5,6,7" --weight-path /data/DeepSeek-R1-AWQ-Marlin/ deep-seek --quant awq --temperature 0. --penalty 1.0 --num-experts-offload-per-rank 15
    ```

    **æ³¨æ„**ï¼šæ­¤è®¾ç½®å°†æ¯ä¸ªrankçš„15ä¸ªä¸“å®¶ï¼ˆæ€»å…±256ä¸ªä¸“å®¶ä¸­çš„120ä¸ªï¼‰å¸è½½åˆ°CPUï¼ˆéœ€è¦çº¦150GBçš„é¢å¤–ä¸»æœºå†…å­˜ï¼‰ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¿™äº›å¸è½½çš„ä¸“å®¶ä¼šæ ¹æ®éœ€è¦äº¤æ¢å›GPUå†…å­˜ã€‚å¦‚æœGPUå†…å­˜æ›´å°‘ï¼Œå¯ä»¥å¢åŠ `--num-experts-offload-per-rank`å‚æ•°ï¼ˆæœ€å¤§æ”¯æŒæ¯ä¸ªrankå¸è½½32ä¸ªä¸“å®¶ï¼‰ã€‚

  </details>

- åœ¨**å¤šèŠ‚ç‚¹**ä¸Šè¿è¡Œ**DeepSeek-R1ï¼ˆ671B/685Bï¼‰**
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **1. å®‰è£…MPIå¹¶æ„å»ºMPIåŠŸèƒ½**
    ```shell
    sudo apt update
    sudo apt install libopenmpi-dev openmpi-bin -y #å®‰è£…MPI
    sudo apt install clang libclang-dev
    #åœ¨ä¸¤ä¸ªèŠ‚ç‚¹çš„ç›¸åŒç›®å½•ä¸‹å…‹éš†ä»“åº“å¹¶æ„å»º
    cargo build --release --features cuda,nccl,mpi #æ„å»ºMPIåŠŸèƒ½
    ```

    **2. å°†AWQ DeepSeekæ¨¡å‹è½¬æ¢ä¸ºMarlinå…¼å®¹æ ¼å¼**
    ```shell
    python3 examples/convert_awq_marlin.py --src /data/DeepSeek-R1-AWQ/ --dst /data/DeepSeek-R1-AWQ-Marlin/ 
    ```

    **3. é…ç½®å¤šèŠ‚ç‚¹ç¯å¢ƒ**

    MPIè¿è¡Œå™¨è¦æ±‚æ‰€æœ‰èŠ‚ç‚¹å…·æœ‰`ç›¸åŒçš„`ç¡¬ä»¶å’Œè½¯ä»¶é…ç½®ï¼Œè¯·ç¡®ä¿æƒé‡å’Œcandle-vllmäºŒè¿›åˆ¶æ–‡ä»¶ä½äºä¸åŒèŠ‚ç‚¹çš„ç›¸åŒæ–‡ä»¶å¤¹ä¸­ã€‚èŠ‚ç‚¹ä¹‹é—´éœ€è¦é€šè¿‡SSHï¼ˆç«¯å£22ï¼‰æ— å¯†ç äº’ç›¸è®¿é—®ï¼ˆå¦‚æœæ˜¯`--allow-run-as-root`åˆ™éœ€è¦rootç”¨æˆ·ï¼‰ã€‚`%NET_INTERFACE%`æ˜¯é€šè¿‡å‘½ä»¤`ifconfig -a`è·å–çš„æ´»åŠ¨ç½‘ç»œæ¥å£ã€‚å¦‚æœèŠ‚ç‚¹ä¸­æ²¡æœ‰InfiniBandï¼Œå¯ä»¥é€šè¿‡æ’å…¥ç¯å¢ƒå˜é‡`-x NCCL_IB_DISABLE=1`æ¥ç¦ç”¨å®ƒã€‚`hostfile`å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼š

    ç¤ºä¾‹ï¼ˆä¸¤ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹8å—GPUï¼‰ï¼š
    ```
    192.168.1.100 slots=8
    192.168.1.101 slots=8
    ```

    **4. ä½¿ç”¨MPIè¿è¡Œå™¨åœ¨ä¸¤ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œæ¨¡å‹**
    ```shell
    sudo mpirun -np 16 -x RUST_LOG=info -hostfile ./hostfile --allow-run-as-root -bind-to none -map-by slot --mca plm_rsh_args "-p 22" --mca btl_tcp_if_include %NET_INTERFACE% target/release/candle-vllm --log --multi-process --dtype bf16 --port 2000 --device-ids "0,1,2,3,4,5,6,7" --weight-path /data/DeepSeek-R1-AWQ-Marlin/ deep-seek --quant awq --temperature 0. --penalty 1.0
    ```
  </details>

- åœ¨å¤šæ ¸CPUæœºå™¨ä¸Šä½¿ç”¨ **NUMAç»‘å®š**è¿è¡Œæ¨¡å‹
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    **å‰ç½®æ¡ä»¶**
    è¯·ç¡®ä¿ä½ çš„æœºå™¨æœ‰å¤šä¸ª NUMA èŠ‚ç‚¹ï¼ˆå³å¤šä¸ªç‰©ç† CPUï¼‰ï¼Œå¹¶å®‰è£… numactlï¼š

    ```shell
    sudo apt-get install numactl
    ```

    å‡è®¾ä½ çš„æœºå™¨æœ‰ 8 å¼  GPU å’Œ 2 ä¸ª NUMA èŠ‚ç‚¹ï¼Œæ¯ 4 å¼  GPU ç»‘å®šåˆ°ä¸€ä¸ªä¸åŒçš„ NUMA èŠ‚ç‚¹ã€‚
    å¦‚æœä½ æƒ³ä½¿ç”¨å…¨éƒ¨ GPU è¿›è¡Œæ¨ç†ï¼Œä¸‹é¢çš„ NUMA ç»‘å®šé…ç½®å¯ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼š

    ```shell
    MAP_NUMA_NODE=0,0,0,0,1,1,1,1 numactl --cpunodebind=0 --membind=0 cargo run --release --features cuda,nccl -- --multi-process --dtype bf16 --port 2000 --device-ids "0,1,2,3,4,5,6,7" --weight-path /home/data/DeepSeek-V2-Chat-AWQ-Marlin deep-seek --quant awq --temperature 0. --penalty 1.0
    ```

    å¦‚æœä½ åªä½¿ç”¨ 4 å¼  GPUï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹çš„ NUMA ç»‘å®šæ–¹å¼ï¼š
    
    ```shell
    MAP_NUMA_NODE=0,0,0,0 numactl --cpunodebind=0 --membind=0 cargo run --release --features cuda,nccl -- --multi-process --dtype bf16 --port 2000 --device-ids "0,1,2,3" --weight-path /home/data/DeepSeek-V2-Chat-AWQ-Marlin deep-seek --quant awq --temperature 0. --penalty 1.0
    ```

    ä»¥ä¸Šå‘½ä»¤ä¸­ `numactl --cpunodebind=0 --membind=0`æŒ‡å®šäº†masterè¿›ç¨‹ï¼ˆmaster rankï¼‰ç»‘å®šçš„NUMA nodeï¼Œå…¶å¿…é¡»ä¸ `MAP_NUMA_NODE`ç›¸åŒ¹é…ã€‚
    
    æ³¨æ„ï¼š ç»‘å®šé¡ºåºå¯èƒ½ä¼šæ ¹æ®ä½ çš„ç¡¬ä»¶é…ç½®æœ‰æ‰€ä¸åŒã€‚
  </details>

- ä½¿ç”¨**Qwen3-Reranker**è¿›è¡ŒçŸ¥è¯†æ£€ç´¢
  <details>
    <summary>æ˜¾ç¤ºå‘½ä»¤</summary>

    1) å¯åŠ¨`Qwen3-Reranker`æ¨¡å‹æœåŠ¡
    ```shell
    target/release/candle-vllm --port 2000 --multi-process --weight-file /home/data/Qwen3-Reranker-4B-q4_k_m.gguf qwen3 --quant gguf
    ```

    2) å¯åŠ¨è¿·ä½ èŠå¤©æœºå™¨äººå¹¶ä¼ å…¥`system prompt`
    ```shell
    python3 examples/chat.py --thinking True --system_prompt "Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\"."
    ```

    3) ä½¿ç”¨query/docå¯¹è¿›è¡ŒçŸ¥è¯†æ£€æŸ¥ï¼Œä¾‹å¦‚ï¼š
    ```shell
    <Query>: What is the capital of China?\n\n<Document>: The capital of China is Beijing.
    ```

    è§‚å¯Ÿè¾“å‡ºç»“æœï¼š
    
    ```shell
    ğŸ™‹ Please Input (Ctrl+C to start a new chat or exit): <Query>: What is the capital of China?\n\n<Document>: The capital of China is Beijing.
    Candle-vLLM: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    <think>
    Okay, the user is asking for the capital of China. The document provided is a direct answer: "The capital of China is Beijing." I need to check if this is correct. From my knowledge, Beijing is indeed the capital of China. The answer is correct and straightforward. The document meets the requirement as it provides the accurate information. So the answer is yes.
    </think>

    yes
    ```
  </details>

## å¦‚ä½•å‘åç«¯å‘é€è¯·æ±‚ï¼Ÿ

**å¯åŠ¨åç«¯æœåŠ¡åè¿è¡ŒèŠå¤©å‰ç«¯**

èŠå¤©å‰ç«¯ï¼ˆä»»ä½•ä¸OpenAI APIå…¼å®¹çš„å‰ç«¯ï¼Œä»¥ä¸‹æ˜¯ç®€å•é€‰é¡¹ï¼‰ï¼š

- **é€‰é¡¹1ï¼šä½¿ç”¨Chat.pyèŠå¤©ï¼ˆç”¨äºç®€å•æµ‹è¯•ï¼‰**
  <details>
    <summary>æ˜¾ç¤ºé€‰é¡¹1</summary>
    
    å®‰è£…APIå’ŒèŠå¤©æœºå™¨äººä¾èµ–ï¼ˆopenaiåŒ…ä»…ç”¨äºä¸candle-vllmæœ¬åœ°èŠå¤©ï¼‰

    ```shell
    python3 -m pip install openai rich click
    ```

    ä½¿ç”¨è¿·ä½ èŠå¤©æœºå™¨äººï¼ˆçº¯æ–‡æœ¬ï¼‰
    ```shell
    python3 examples/chat.py
    ```

    ä¼ é€’ç”Ÿæˆå‚æ•°ï¼ˆä½¿ç”¨`--thinking True`ä¸æ¨ç†æ¨¡å‹äº¤äº’ï¼‰
    ```shell
    python3 examples/chat.py --temperature 0.7 --top_k 64 --top_p 0.9 --thinking True --system_prompt "Thinking big!"
    ```

    ä½¿ç”¨è¿·ä½ èŠå¤©æœºå™¨äººï¼ˆå®æ—¶æ›´æ–°Markdownï¼Œå¯èƒ½ä¼šé—ªçƒï¼‰
    ```shell
    python3 examples/chat.py --live
    ```

    æ³¨ï¼š**å¼€å¯VPNçŠ¶æ€ä¸‹ï¼ŒVPNæœåŠ¡å¯èƒ½ä¼šå½±å“æœ¬åœ°èŠå¤©è¯·æ±‚**
  <details>

- **é€‰é¡¹2ï¼šä½¿ç”¨ç®€å•çš„ChatUIï¼ˆæˆ–æµè¡Œçš„difyå‰ç«¯ï¼‰**
  <details>
    <summary>æ˜¾ç¤ºé€‰é¡¹2</summary>

    å®‰è£…ç®€å•çš„ChatUIåŠå…¶ä¾èµ–ï¼š

    ```
    git clone git@github.com:guoqingbao/candle-vllm-demo.git
    cd candle-vllm-demo
    apt install npm #å¦‚æœéœ€è¦ï¼Œå®‰è£…npm
    npm install n -g #å¦‚æœéœ€è¦ï¼Œæ›´æ–°Node.js
    n stable #å¦‚æœéœ€è¦ï¼Œæ›´æ–°Node.js
    npm i -g pnpm #å®‰è£…pnpmç®¡ç†å™¨
    pnpm install #å®‰è£…ChatUIä¾èµ–
    ```

    å¯åŠ¨ChatUIï¼š
    ```
    pnpm run dev #è¿è¡ŒChatUI
    ```

    **Nodejsé”™è¯¯å¤„ç†**
    `ENOSPC: System limit for number of file watchers reached`
    ```
    echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p
    ```
  </details>

- **é€‰é¡¹3ï¼šé€šè¿‡HTTP postå‘é€èŠå¤©å®Œæˆè¯·æ±‚**
  <details>
    <summary>æ˜¾ç¤ºé€‰é¡¹3</summary>

    ``` shell
    curl -X POST "http://127.0.0.1:2000/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer YOUR_API_KEY" \
        -d '{
            "model": "llama7b",
            "messages": [
                {"role": "user", "content": "è§£é‡Šå¦‚ä½•æœ€å¥½åœ°å­¦ä¹ Rustã€‚"}
            ],
            "temperature": 0.7,
            "max_tokens": 128,
            "stop": {"Single":"</s>"}
        }'
    ```
    ç¤ºä¾‹å“åº”ï¼š

    ```
    {"id":"cmpl-53092967-c9cf-40e0-ae26-d7ac786d59e8","choices":[{"message":{"content":" å­¦ä¹ ä»»ä½•ç¼–ç¨‹è¯­è¨€éƒ½éœ€è¦ç†è®ºã€å®è·µå’Œä¸“æ³¨çš„ç»“åˆã€‚ä»¥ä¸‹æ˜¯å¸®åŠ©ä½ é«˜æ•ˆå­¦ä¹ Rustçš„ä¸€äº›æ­¥éª¤å’Œèµ„æºï¼š\n\n1. ä»åŸºç¡€å¼€å§‹ï¼š\n\t* ç†è§£Rustç¨‹åºçš„è¯­æ³•å’ŒåŸºæœ¬ç»“æ„ã€‚\n\t* å­¦ä¹ å˜é‡ã€æ•°æ®ç±»å‹ã€å¾ªç¯å’Œæ§åˆ¶ç»“æ„ã€‚\n\t* ç†Ÿæ‚‰Rustçš„æ‰€æœ‰æƒç³»ç»Ÿå’Œå€Ÿç”¨æœºåˆ¶ã€‚\n2. é˜…è¯»Rustå®˜æ–¹ä¹¦ç±ï¼š\n\t* Rustå®˜æ–¹ä¹¦ç±æ˜¯å…¨é¢ä»‹ç»è¯¥è¯­è¨€çš„èµ„æºã€‚\n\t* å®ƒæ¶µç›–äº†è¯¸å¦‚","role":"[INST]"},"finish_reason":"length","index":0,"logprobs":null}],"created":1718784498,"model":"llama7b","object":"chat.completion","usage":{"completion_tokens":129,"prompt_tokens":29,"total_tokens":158}}
    ```
  </details>

- **é€‰é¡¹4ï¼šä½¿ç”¨openaiåŒ…å‘é€èŠå¤©å®Œæˆè¯·æ±‚**
  <details>
    <summary>æ˜¾ç¤ºé€‰é¡¹4</summary>

    åœ¨ç»ˆç«¯ä¸­è¿è¡Œ`pip install openai`å®‰è£…`openai` PythonåŒ…ã€‚è¿™é‡Œä½¿ç”¨çš„æ˜¯`1.3.5`ç‰ˆæœ¬ã€‚

    ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„Pythonæ–‡ä»¶å¹¶å†™å…¥ä»¥ä¸‹ä»£ç ï¼š
    ```python
    import openai

    openai.api_key = "EMPTY"

    openai.base_url = "http://localhost:2000/v1/"

    completion = openai.chat.completions.create(
        model="llama",
        messages=[
            {
                "role": "user",
                "content": "è§£é‡Šå¦‚ä½•æœ€å¥½åœ°å­¦ä¹ Rustã€‚",
            },
        ],
        max_tokens = 64,
    )
    print(completion.choices[0].message.content)
    ```
    åœ¨`candle-vllm`æœåŠ¡è¿è¡Œåï¼Œè¿è¡ŒPythonè„šæœ¬å³å¯äº«å—é«˜æ•ˆçš„æ¨ç†æœåŠ¡ï¼


    **æ‰¹é‡è¯·æ±‚**

    é¦–å…ˆå®‰è£…openai APIï¼š
    ```
    python3 -m pip install openai
    ```

    è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼š
    ``` shell
    python3 examples/benchmark.py --batch 16 --max_tokens 1024
    ```
    å‚è€ƒ`examples/benchmark.py`ï¼š

    ``` python
    async def benchmark():
        model = "mistral7b"
        max_tokens = 1024
        # 16ä¸ªè¯·æ±‚
        prompts = ["è§£é‡Šå¦‚ä½•æœ€å¥½åœ°å­¦ä¹ Rustã€‚", 
                "è¯·ç”¨100å­—è°ˆè°ˆæ·±åº¦å­¦ä¹ ã€‚", 
                "ä½ çŸ¥é“ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªä¸ªåŸå¸‚å—ï¼Ÿè°ˆè°ˆä½ æ‰€çŸ¥é“çš„ç»†èŠ‚ã€‚", 
                "è°æ˜¯ä¸–ç•Œä¸Šæœ€å¥½çš„å¥³æ¼”å‘˜ï¼Ÿè§£é‡ŠåŸå› ã€‚",
                "å¦‚ä½•åº”å¯¹æŠ‘éƒç—‡ï¼Ÿ",
                "å¦‚ä½•åœ¨çŸ­æ—¶é—´å†…èµšé’±ï¼Ÿ",
                "å¤§è¯­è¨€æ¨¡å‹çš„æœªæ¥è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
                "ä¸–ç•Œä¸Šè‘—åçš„ç§‘æŠ€å…¬å¸ã€‚",
                "è§£é‡Šå¦‚ä½•æœ€å¥½åœ°å­¦ä¹ Rustã€‚", 
                "è¯·ç”¨100å­—è°ˆè°ˆæ·±åº¦å­¦ä¹ ã€‚", 
                "ä½ çŸ¥é“ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªä¸ªåŸå¸‚å—ï¼Ÿè°ˆè°ˆä½ æ‰€çŸ¥é“çš„ç»†èŠ‚ã€‚", 
                "è°æ˜¯ä¸–ç•Œä¸Šæœ€å¥½çš„å¥³æ¼”å‘˜ï¼Ÿè§£é‡ŠåŸå› ã€‚",
                "å¦‚ä½•åº”å¯¹æŠ‘éƒç—‡ï¼Ÿ",
                "å¦‚ä½•åœ¨çŸ­æ—¶é—´å†…èµšé’±ï¼Ÿ",
                "å¤§è¯­è¨€æ¨¡å‹çš„æœªæ¥è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
                "ä¸–ç•Œä¸Šè‘—åçš„ç§‘æŠ€å…¬å¸ã€‚"]
        
        # åŒæ—¶å‘é€16ä¸ªèŠå¤©è¯·æ±‚
        tasks: List[asyncio.Task] = []
        for i in range(len(prompts)):
            tasks.append(
                asyncio.create_task(
                    chat_completion(model, max_tokens, prompts[i]))
            )

        # è·å–æ¯ä¸ªè¯·æ±‚çš„æµå¯¹è±¡
        outputs: List[Stream[ChatCompletionChunk]] = await asyncio.gather(*tasks)

        # æµå¼èŠå¤©å“åº”çš„ä»»åŠ¡
        tasks_stream: List[asyncio.Task] = []
        for i in range(len(outputs)):
            tasks_stream.append(
                asyncio.create_task(
                    stream_response(i, outputs[i]))
            )

        # æ”¶é›†å“åº”æ–‡æœ¬
        outputs: List[(int, str)] = await asyncio.gather(*tasks_stream)

        # æ‰“å°ç»“æœï¼Œå¯ä»¥åœ¨åç«¯æœåŠ¡å™¨ï¼ˆå³candle-vllmï¼‰ä¸­æŸ¥çœ‹èŠå¤©å®Œæˆç»Ÿè®¡ä¿¡æ¯
        for idx, output in outputs:
            print("\n\n å“åº” {}: \n\n {}".format(idx, output))


    asyncio.run(benchmark())
    ```
  </details>

## åŸä½ï¼ˆin-situï¼‰é‡åŒ–
- **åŸä½é‡åŒ–å’ŒåŸä½Marlinæ ¼å¼è½¬æ¢**
  <details>
    <summary>æ˜¾ç¤ºé‡åŒ–é…ç½®</summary>

    Candle-vllmæ”¯æŒåœ¨æ¨¡å‹åŠ è½½æ—¶å°†é»˜è®¤æƒé‡ï¼ˆF32/F16/BF16ï¼‰è½¬æ¢ä¸ºä»»ä½•GGML/GGUFæ ¼å¼ï¼Œæˆ–å°†`4ä½GPTQ/AWQ`æƒé‡è½¬æ¢ä¸º`Marlin`æ ¼å¼è¿›è¡ŒåŠ é€Ÿã€‚æ­¤åŠŸèƒ½æœ‰åŠ©äºèŠ‚çœGPUå†…å­˜ï¼ˆæˆ–é€šè¿‡Marlinå†…æ ¸åŠ é€Ÿæ¨ç†æ€§èƒ½ï¼‰ï¼Œä½¿å…¶æ›´é€‚åˆæ¶ˆè´¹çº§GPUï¼ˆä¾‹å¦‚RTX 4090ï¼‰ã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåªéœ€åœ¨è¿è¡Œcandle-vllmæ—¶ä¼ é€’ç›¸åº”`quant`å‚æ•°ã€‚

    **å¯¹äºæœªé‡åŒ–æ¨¡å‹ï¼š**

    ```
    cargo run --release --features cuda -- --port 2000 --weight-path /home/Meta-Llama-3.1-8B-Instruct/ llama3 --quant q4k
    ```

    `quant`å‚æ•°é€‰é¡¹ï¼š["q4_0", "q4_1", "q5_0", "q5_1", "q8_0", "q2k", "q3k","q4k","q5k","q6k"]

    **å¯¹äº4ä½GPTQé‡åŒ–æ¨¡å‹ï¼š**

    ```
    cargo run --release --features cuda -- --port 2000 --weight-path /home/mistral_7b-int4/ mistral --quant marlin
    ```

    **å…³äºMarlinçš„æ³¨æ„äº‹é¡¹**ï¼š

    1) å°†F32/F16/BF16æ¨¡å‹åŠ è½½ä¸ºé‡åŒ–æ ¼å¼å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼›

    2) åŸä½Marlinæ ¼å¼è½¬æ¢ä»…æ”¯æŒ4ä½GPTQï¼ˆ`sym=True`ï¼Œ`groupsize=128`æˆ–-1ï¼Œ`desc_act=False`ï¼‰å’Œ4ä½AWQï¼ˆä½¿ç”¨ç»™å®šè„šæœ¬è½¬æ¢åï¼‰ï¼›

    3) Marlinæ ¼å¼ä»…åœ¨CUDAå¹³å°ä¸Šæ”¯æŒã€‚
  </details>

## å…¶ä»–ç”¨æ³•
- KVç¼“å­˜é…ç½®ã€é‡‡æ ·å‚æ•°ç­‰

  <details>
    <summary>æ˜¾ç¤ºè¯¦æƒ…</summary>
    å¯¹äºKVç¼“å­˜é…ç½®ï¼Œè®¾ç½®`kvcache-mem-gpu`ï¼Œé»˜è®¤ä¸º4GB GPUå†…å­˜ç”¨äºKVç¼“å­˜ï¼Œé•¿æ–‡æœ¬æˆ–æ‰¹é‡æ¨ç†æ—¶è¯·å¢å¤§KVç¼“å­˜ã€‚

    å¯¹äºèŠå¤©å†å²è®¾ç½®ï¼Œå°†`record_conversation`è®¾ç½®ä¸º`true`ä»¥è®©candle-vllmè®°ä½èŠå¤©å†å²ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œcandle-vllm`ä¸ä¼š`è®°å½•èŠå¤©å†å²ï¼›ç›¸åï¼Œå®¢æˆ·ç«¯å°†æ¶ˆæ¯å’Œä¸Šä¸‹æ–‡å†å²ä¸€èµ·å‘é€ç»™candle-vllmã€‚å¦‚æœ`record_conversation`è®¾ç½®ä¸º`true`ï¼Œå®¢æˆ·ç«¯ä»…å‘é€æ–°çš„èŠå¤©æ¶ˆæ¯ç»™candle-vllmï¼Œè€Œcandle-vllmè´Ÿè´£è®°å½•ä¹‹å‰çš„èŠå¤©æ¶ˆæ¯ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éœ€è¦æ¯ä¼šè¯èŠå¤©è®°å½•ï¼Œç›®å‰å°šæœªå®ç°ï¼Œå› æ­¤æ¨èä½¿ç”¨é»˜è®¤æ–¹æ³•`record_conversation=false`ã€‚

    å¯¹äºèŠå¤©æµå¼ä¼ è¾“ï¼Œéœ€è¦åœ¨èŠå¤©è¯·æ±‚ä¸­å°†`stream`æ ‡å¿—è®¾ç½®ä¸º`True`ã€‚

    ä½ å¯ä»¥ä¼ é€’`penalty`å’Œ`temperature`å‚æ•°ç»™æ¨¡å‹ä»¥**é˜²æ­¢æ½œåœ¨çš„é‡å¤**ï¼Œä¾‹å¦‚ï¼š

    ```
    cargo run --release --features cuda -- --port 2000 --weight-path /home/mistral_7b/ mistral --repeat-last-n 64 --penalty 1.1 --temperature 0.7
    ```

    `--max-gen-tokens`å‚æ•°ç”¨äºæ§åˆ¶æ¯æ¬¡èŠå¤©å“åº”çš„æœ€å¤§è¾“å‡ºä»¤ç‰Œæ•°ã€‚é»˜è®¤å€¼å°†è®¾ç½®ä¸º`max_sequence_len`çš„1/5ã€‚

    å¯¹äº`æ¶ˆè´¹çº§GPU`ï¼Œå»ºè®®ä»¥GGMLæ ¼å¼ï¼ˆæˆ–Marlinæ ¼å¼ï¼‰è¿è¡Œæ¨¡å‹ï¼Œä¾‹å¦‚ï¼š

    ```
    cargo run --release --features cuda -- --port 2000 --weight-path /home/Meta-Llama-3.1-8B-Instruct/ llama3 --quant q4k
    ```

    å…¶ä¸­`quant`å¯é€‰å€¼ä¸ºï¼š["q4_0", "q4_1", "q5_0", "q5_1", "q8_0", "q2k", "q3k","q4k","q5k","q6k", "awq", "gptq", "marlin", "gguf", "ggml"]ã€‚
  </details>

- **GPTQ/AWQæ¨¡å‹é€šè¿‡Marlin KernelåŠ é€Ÿ**
  <details>
    <summary>æ˜¾ç¤ºè¯¦æƒ…</summary>

    Candle-vllmç°åœ¨æ”¯æŒGPTQ/AWQï¼ˆMarlinå†…æ ¸ï¼‰ï¼Œå¦‚æœä½ æœ‰`Marlin`æ ¼å¼çš„é‡åŒ–æƒé‡ï¼Œå¯ä»¥ä¼ é€’`quant`ï¼ˆmarlinï¼‰å‚æ•°ï¼Œä¾‹å¦‚ï¼š

    ```shell
    cargo run --release --features cuda -- --port 2000 --dtype f16 --weight-path /home/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4-Marlin/ llama3 --quant marlin --temperature 0. --penalty 1.
    ```

    æˆ–è€…ï¼Œå°†ç°æœ‰çš„AWQ 4ä½æ¨¡å‹è½¬æ¢ä¸ºMarlinå…¼å®¹æ ¼å¼ï¼š
    ```shell
    python3 examples/convert_awq_marlin.py --src /home/Meta-Llama-3.1-8B-Instruct-AWQ-INT4/ --dst /home/Meta-Llama-3.1-8B-Instruct-AWQ-INT4-Marlin/ --bits 4 --method awq --group 128 --nk False
    cargo run --release --features cuda,nccl -- --multi-process --dtype f16 --port 2000 --device-ids "0" --weight-path /home/Meta-Llama-3.1-8B-Instruct-AWQ-INT4-Marlin/ llama3 --quant awq --temperature 0. --penalty 1.0
    ```

    ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨`GPTQModel`é€šè¿‡è„šæœ¬`examples/convert_marlin.py`å°†æ¨¡å‹è½¬æ¢ä¸ºMarlinå…¼å®¹æ ¼å¼ã€‚

    **æ³¨æ„**ï¼šç›®å‰ä»…æ”¯æŒ4ä½GPTQé‡åŒ–çš„Marlinå¿«é€Ÿå†…æ ¸ã€‚
  </details>

## æŠ¥å‘Šé—®é¢˜
å®‰è£…`candle-vllm`éå¸¸ç®€å•ï¼Œåªéœ€æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œã€‚å¦‚æœé‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·åˆ›å»º[issue](https://github.com/EricLBuehler/candle-vllm/issues)ã€‚


## å‚è€ƒ
- Pythonå®ç°ï¼š[`vllm-project`](https://github.com/vllm-project/vllm)
- [`vllm`è®ºæ–‡](https://arxiv.org/abs/2309.06180)