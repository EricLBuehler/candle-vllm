# Example models.yaml configuration file for candle-vllm
#
# This file defines model aliases and their configurations.
# Set CANDLE_VLLM_MODELS_CONFIG environment variable to point to your models.yaml file,
# or place it as 'models.yaml' or 'models.yml' in the current directory.
#
# Example usage:
#   export CANDLE_VLLM_MODELS_CONFIG=/path/to/models.yaml
#   cargo run --release --features metal -- --m mistral-7b
#
# The model alias (e.g., "mistral-7b") can then be used with the --m flag:
#   cargo run --release --features metal -- --m mistral-7b

# Optional: Auto-unload models after being idle for this many seconds
# Set to null or omit to disable auto-unloading
# Models will be automatically unloaded if no requests are received for this duration
idle_unload_secs: 3600  # 1 hour

# ============================================================================
# Parking Lot Scheduler Configuration (Optional)
# ============================================================================
# Configure the thread pool and resource scheduler for inference.
# If omitted, sensible defaults are used based on system resources.

parking_lot:
  # Thread pool for CPU-bound inference work
  pool:
    # Number of worker threads for model inference (default: num_cpus)
    # Each worker thread can process one inference request at a time
    worker_threads: 4
    
    # Maximum blocking threads for tokio runtime (default: 512)
    # Used for I/O operations and async tasks
    max_blocking_threads: 512
    
    # Stack size per worker thread in bytes (default: 2MB)
    thread_stack_size: 2097152
    
  # Resource limits and queue configuration
  limits:
    # Maximum resource units (GPU KV-cache blocks)
    # Set to null to auto-derive from kvcache_mem_gpu configuration
    # This determines how many requests can run concurrently
    max_units: null
    
    # Maximum number of queued requests before rejection
    # Requests beyond this limit will be rejected with 503 error
    max_queue_depth: 1000
    
    # Request timeout in seconds
    # Requests that don't complete within this time are cancelled
    timeout_secs: 120
    
  # Queue backend configuration
  queue:
    # Backend type for task queue
    # Options: "memory" (default, non-persistent)
    #          "postgres" (persistent, requires postgres_url)
    #          "yaque" (persistent file-based, requires yaque_dir)
    backend: "memory"
    
    # Enable persistent queue (survives server restarts)
    # Only works with postgres or yaque backends
    persistence: false
    
    # PostgreSQL connection string (required if backend = "postgres")
    # postgres_url: "postgresql://user:password@localhost:5432/candle_vllm"
    
    # Directory for Yaque file-based queue (required if backend = "yaque")
    # yaque_dir: "./queue_data"
    
  # Result mailbox configuration
  mailbox:
    # Backend type for storing completed results
    # Options: "memory" (default, non-persistent)
    #          "postgres" (persistent, requires postgres_url)
    backend: "memory"
    
    # How long to retain completed results in seconds
    # After this time, results are purged from the mailbox
    retention_secs: 3600  # 1 hour
    
    # PostgreSQL connection string (required if backend = "postgres")
    # postgres_url: "postgresql://user:password@localhost:5432/candle_vllm"

models:
  # Example: Hugging Face model with basic parameters
  - name: mistral-7b
    hf_id: mistralai/Mistral-7B-Instruct-v0.3
    params:
      dtype: f16
      temperature: 0.7
      top_p: 0.9
      top_k: 40
      frequency_penalty: 0.0
      presence_penalty: 0.0
      kvcache_mem_gpu: 8192  # MB
      max_num_seqs: 256
    notes: "Mistral 7B Instruct model from Hugging Face"

  # Example: Local model path
  - name: local-mistral
    local_path: ./models/mistral-7b-instruct
    params:
      dtype: f16
      quantization: q4k
      kvcache_mem_gpu: 4096
      max_num_seqs: 128
    notes: "Local Mistral model with quantization"

  # Example: Model with specific weight file
  - name: custom-model
    hf_id: microsoft/phi-2
    weight_file: model.safetensors
    params:
      dtype: f16
      block_size: 16
      kvcache_mem_gpu: 2048
      device_ids: [0]  # Use GPU 0
    notes: "Custom model configuration"

  # Example: Multi-GPU model with custom parking lot settings
  - name: large-model
    hf_id: mistralai/Mixtral-8x7B-Instruct-v0.1
    params:
      dtype: f16
      device_ids: [0, 1, 2, 3]  # Use GPUs 0-3
      kvcache_mem_gpu: 16384
      max_num_seqs: 512
      multithread: true
    # Per-model parking lot overrides (optional)
    # These override the global parking_lot settings for this specific model
    parking_lot:
      limits:
        max_units: 4096         # Higher limit for large model
        max_queue_depth: 2000   # Larger queue
        timeout_secs: 300       # Longer timeout for complex requests
    notes: "Large model distributed across multiple GPUs with custom resource limits"

  # Example: CPU-only model
  - name: cpu-model
    hf_id: microsoft/phi-2
    params:
      dtype: f32
      kvcache_mem_cpu: 4096
      device_ids: []  # Empty = CPU
    notes: "Model running on CPU"

  # Example: Model with in-situ quantization
  - name: quantized-model
    hf_id: mistralai/Mistral-7B-Instruct-v0.3
    params:
      dtype: f16
      isq: q4k  # In-situ quantization
      kvcache_mem_gpu: 4096
      max_num_seqs: 128
    notes: "Model with in-situ quantization enabled"

  # Example: Model with prefill chunking
  - name: chunked-model
    hf_id: mistralai/Mistral-7B-Instruct-v0.3
    params:
      dtype: f16
      prefill_chunk_size: 512
      kvcache_mem_gpu: 8192
      max_num_seqs: 256
    notes: "Model with chunked prefill for better throughput"

# ============================================================================
# Parameter Reference
# ============================================================================
#
# Model-level fields:
#   name:          Alias name for the model (used with --m <name>)
#   hf_id:         Hugging Face model identifier (e.g., "mistralai/Mistral-7B-Instruct-v0.3")
#   local_path:    Local path to model files (alternative to hf_id)
#   weight_file:   Specific weight file to load (optional, for hf_id or local_path)
#   params:        Model execution parameters (see below)
#   notes:         Optional description/notes about the model
#
# Parameters (under "params" key):
#   dtype:              Data type (f32, f16, bf16)
#   quantization:       Quantization method (q4k, q8_0, etc.) - applied at load time
#   isq:                In-situ quantization (q4k, q8_0, etc.) - applied during inference
#   block_size:         KV cache block size (default: 64)
#   max_num_seqs:       Maximum number of sequences in batch
#   kvcache_mem_gpu:    KV cache memory in MB (GPU)
#   kvcache_mem_cpu:    KV cache memory in MB (CPU)
#   prefill_chunk_size: Chunk size for prefill (optional, improves throughput)
#   multithread:        Enable multi-threading (true/false)
#   device_ids:         List of GPU device IDs to use (empty [] = CPU)
#   temperature:        Sampling temperature (0.0-2.0, default: 0.7)
#   top_p:              Nucleus sampling parameter (0.0-1.0)
#   top_k:              Top-k sampling parameter (positive integer or -1 for disabled)
#   frequency_penalty:  Frequency penalty (-2.0 to 2.0)
#   presence_penalty:   Presence penalty (-2.0 to 2.0)
#
# Parking Lot Configuration Reference:
#   pool.worker_threads:      Number of threads for CPU-bound inference (default: num_cpus)
#   pool.max_blocking_threads: Max tokio blocking threads (default: 512)
#   pool.thread_stack_size:   Stack size per thread in bytes (default: 2MB)
#   limits.max_units:         Max KV-cache blocks (null = auto from config)
#   limits.max_queue_depth:   Max queued requests before rejection
#   limits.timeout_secs:      Request timeout in seconds
#   queue.backend:            "memory" | "postgres" | "yaque"
#   queue.persistence:        Enable persistent queue (bool)
#   mailbox.backend:          "memory" | "postgres"
#   mailbox.retention_secs:   How long to keep results (seconds)
#
# Model Configuration Notes:
#   - Either "hf_id" or "local_path" must be specified (not both)
#   - "weight_file" is optional and used to specify a specific file when multiple exist
#   - All sampling parameters (temperature, top_p, etc.) are optional and have defaults
#   - Memory parameters (kvcache_mem_gpu, kvcache_mem_cpu) are in megabytes
#   - For CPU-only models, set device_ids to [] and use kvcache_mem_cpu
#   - For multi-GPU models, specify multiple device_ids and enable multithread
#   - quantization and isq are different: quantization is applied at load time,
#     while isq (in-situ quantization) is applied during inference
#   - Per-model "parking_lot" section overrides global parking_lot settings
