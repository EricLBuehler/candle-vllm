# Test Environment Configuration for candle-vllm Integration Tests
#
# IMPORTANT: For real inference tests to work, models must:
# 1. Have HF_TOKEN set (get from https://huggingface.co/settings/tokens)
# 2. Use models with standard weight file naming:
#    - model.safetensors (single file), or
#    - model.safetensors.index.json with model-XXXXX-of-YYYYY.safetensors (sharded)
#
# Some models (e.g., those with consolidated.safetensors) may not load correctly
# until they are properly converted or the loader is updated.

# ============================================================================
# Model Configuration
# ============================================================================

# Path to test.models.yaml (relative to workspace root or absolute)
# If not set, defaults to workspace_root/test.models.yaml
CANDLE_VLLM_TEST_MODELS_CONFIG=test.models.yaml

# Path to MCP configuration file for tests
# If not set, defaults to workspace_root/mcp.json
CANDLE_VLLM_TEST_MCP_CONFIG=mcp.json

# ============================================================================
# HuggingFace Configuration  
# ============================================================================

# HuggingFace API token for downloading models
# Get your token from: https://huggingface.co/settings/tokens
# Option 1: Set the token directly (not recommended for shared files)
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Option 2: Use a token file path (recommended)
# Default macOS/Linux location: ~/.cache/huggingface/token
# This is where `huggingface-cli login` stores the token
HF_TOKEN_PATH=~/.cache/huggingface/token

# HuggingFace cache directory (default: ~/.cache/huggingface)
# HF_HOME=~/.cache/huggingface

# ============================================================================
# Device Configuration
# ============================================================================

# Device to use for tests: "metal", "cuda", or "cpu"
# For M1/M2/M3 Macs, use "metal"
# For NVIDIA GPUs, use "cuda"
CANDLE_VLLM_TEST_DEVICE=metal

# GPU device IDs to use (comma-separated for multi-GPU)
# CANDLE_VLLM_TEST_DEVICE_IDS=0

# ============================================================================
# Test Data Paths
# ============================================================================

# Path to test files directory (for images, etc.)
# If not set, defaults to workspace_root/test-files
CANDLE_VLLM_TEST_FILES_DIR=test-files

# ============================================================================
# Test Behavior
# ============================================================================

# Skip tests that require model downloads (set to "true" to skip)
# CANDLE_VLLM_SKIP_DOWNLOAD_TESTS=false

# Timeout for model loading in seconds (default: 600)
CANDLE_VLLM_TEST_MODEL_TIMEOUT=600

# Enable verbose test output
# CANDLE_VLLM_TEST_VERBOSE=true

# ============================================================================
# Logging
# ============================================================================

# Log level for tests: trace, debug, info, warn, error
RUST_LOG=info,candle_vllm_core=debug,candle_vllm_server=debug

