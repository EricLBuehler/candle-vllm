idle_unload_secs: 3600  # 1 hour

# Default model to use if no model is specified via CLI arguments
default_model: mistral-3-ministral-3B-reasoning

models:
  - name: mistral-3-ministral-3B-reasoning
    hf_id: mistralai/Ministral-3-3B-Reasoning-2512
    params:
      dtype: f16
      temperature: 0.3
      top_p: 0.9
      top_k: 40
      frequency_penalty: 0.0
      presence_penalty: 0.0
      kvcache_mem_gpu: 8192  # MB - Use GPU memory for Metal acceleration
      max_num_seqs: 128  # Increased batch size for better throughput with 64GB RAM
      block_size: 64  # Default block size, optimal for Metal
      device_ids: [0]  # Use Metal GPU (M1 unified memory)
      multithread: false  # Single-threaded is optimal for Metal on M1
    # Text-only model capabilities
    capabilities:
      vision_mode: disabled
    notes: "Mistral 3 Ministral-3-3B-Reasoning-2512 optimized for M1 Mac with 64GB RAM and Metal GPU acceleration"

  - name: phi-3.5-vision-instruct
    hf_id: microsoft/Phi-3.5-vision-instruct
    params:
      dtype: f16
      temperature: 0.7
      top_p: 0.9
      top_k: 50
      frequency_penalty: 0.0
      presence_penalty: 0.0
      kvcache_mem_gpu: 6144  # MB - Smaller KV cache for vision model on M1
      max_num_seqs: 64  # Reduced batch size for vision processing
      block_size: 64  # Default block size, optimal for Metal
      device_ids: [0]  # Use Metal GPU (M1 unified memory)
      multithread: false  # Single-threaded is optimal for Metal on M1
    # Vision model capabilities
    capabilities:
      vision_mode: proxy
      vision_proxy:
        hf_id: microsoft/Phi-3.5-vision-instruct
        prompt_template: "Describe this image in detail:"
    notes: "Phi-3.5 Vision Instruct model for vision tasks, optimized for M1 Mac with 64GB RAM"